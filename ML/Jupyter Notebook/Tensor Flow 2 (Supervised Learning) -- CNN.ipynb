{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALISING CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the image need to be provided in the 2d array format we need to provide the features and the matrix format.\n",
    "\n",
    "Need to Mention these in the convolution 2d matrix object.\n",
    "\n",
    "The basic picture features start from 32 and continues by multiplying with 2.\n",
    "\n",
    "Need to the mention the rows and columns of matrix format aswell.\n",
    "\n",
    "Borders, if not mentioned it will consider default format.\n",
    "\n",
    "Input shape (all images dont have same format)\n",
    "    * 3 be the shape of array (color image)\n",
    "    * 64 be the number of colors, can be increased twice depending on image\n",
    "    \n",
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64,64,3), activation = 'relu')) # 32 features of 3x3 matrix # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POOLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to mention the matrix size of the pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLATTENING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to mention the object for the flattening as the tensorflow keras will consider it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL CONNECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of units should be considered by trail & error.\n",
    "\n",
    "The Rectifier Activation function is used in to verify the data provided.\n",
    "\n",
    "The Sigmoid Activation function is used in order to classify the outputs. If there are two outputs then the activation should be of SIGMOID else SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of loss varies on the number of outcomes.\n",
    "    * If there are just two outcomes, we need to use \"binary_crossentropy\"\n",
    "    * If there are just more than two outcomes, we need to use \"categorical_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITTING CNN TO THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below template is taken from Keras Documentation Which provides all the data required for the output classification.\n",
    "\n",
    "The training and test data is splitted because of the easier processing of data.\n",
    "\n",
    "We need to edit the configurations of the data taken from keras as per our requirement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                        rescale = 1./255,\n",
    "                        shear_range = 0.2,\n",
    "                        zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "                        'G:/Education/Machine Learning/ML_CSV_FILES/CNN Dataset Cats & Dogs/training_set',\n",
    "                        target_size = (64, 64),\n",
    "                        batch_size = 32,\n",
    "                        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "                        'G:/Education/Machine Learning/ML_CSV_FILES/CNN Dataset Cats & Dogs/test_set',\n",
    "                        target_size = (64, 64),\n",
    "                        batch_size = 32,\n",
    "                        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "63/63 [==============================] - 22s 351ms/step - loss: 0.6356 - acc: 0.6335\n",
      "250/250 [==============================] - 120s 480ms/step - loss: 0.6705 - acc: 0.5817 - val_loss: 0.6356 - val_acc: 0.6335\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 11s 174ms/step - loss: 0.6161 - acc: 0.6740\n",
      "250/250 [==============================] - 70s 278ms/step - loss: 0.6086 - acc: 0.6651 - val_loss: 0.6161 - val_acc: 0.6740\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 13s 211ms/step - loss: 0.5715 - acc: 0.7095\n",
      "250/250 [==============================] - 84s 334ms/step - loss: 0.5792 - acc: 0.6991 - val_loss: 0.5715 - val_acc: 0.7095\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 15s 231ms/step - loss: 0.6766 - acc: 0.6175\n",
      "250/250 [==============================] - 97s 387ms/step - loss: 0.5601 - acc: 0.7116 - val_loss: 0.6766 - val_acc: 0.6175\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 0.5751 - acc: 0.7115\n",
      "250/250 [==============================] - 97s 387ms/step - loss: 0.5457 - acc: 0.7239 - val_loss: 0.5751 - val_acc: 0.7115\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.5320 - acc: 0.7340\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.5242 - acc: 0.7361 - val_loss: 0.5320 - val_acc: 0.7340\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.5392 - acc: 0.7355\n",
      "250/250 [==============================] - 104s 416ms/step - loss: 0.5201 - acc: 0.7434 - val_loss: 0.5392 - val_acc: 0.7355\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.5479 - acc: 0.7370\n",
      "250/250 [==============================] - 107s 426ms/step - loss: 0.5025 - acc: 0.7508 - val_loss: 0.5479 - val_acc: 0.7370\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - 16s 253ms/step - loss: 0.5214 - acc: 0.7490\n",
      "250/250 [==============================] - 109s 435ms/step - loss: 0.4926 - acc: 0.7596 - val_loss: 0.5214 - val_acc: 0.7490\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - 17s 273ms/step - loss: 0.6026 - acc: 0.7120\n",
      "250/250 [==============================] - 112s 449ms/step - loss: 0.4865 - acc: 0.7635 - val_loss: 0.6026 - val_acc: 0.7120\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - 17s 271ms/step - loss: 0.5290 - acc: 0.7405\n",
      "250/250 [==============================] - 110s 442ms/step - loss: 0.4807 - acc: 0.7713 - val_loss: 0.5290 - val_acc: 0.7405\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - 21s 336ms/step - loss: 0.5220 - acc: 0.7460\n",
      "250/250 [==============================] - 119s 475ms/step - loss: 0.4751 - acc: 0.7642 - val_loss: 0.5220 - val_acc: 0.7460\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 0.5383 - acc: 0.7535\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.4625 - acc: 0.7778 - val_loss: 0.5383 - val_acc: 0.7535\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - 17s 274ms/step - loss: 0.5707 - acc: 0.7305\n",
      "250/250 [==============================] - 118s 471ms/step - loss: 0.4567 - acc: 0.7799 - val_loss: 0.5707 - val_acc: 0.7305\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.5189 - acc: 0.7545\n",
      "250/250 [==============================] - 109s 436ms/step - loss: 0.4431 - acc: 0.7934 - val_loss: 0.5189 - val_acc: 0.7545\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.5311 - acc: 0.7565\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.4438 - acc: 0.7874 - val_loss: 0.5311 - val_acc: 0.7565\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - 17s 266ms/step - loss: 0.5147 - acc: 0.7605\n",
      "250/250 [==============================] - 110s 440ms/step - loss: 0.4300 - acc: 0.7920 - val_loss: 0.5147 - val_acc: 0.7605\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - 19s 303ms/step - loss: 0.5447 - acc: 0.7630\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.4222 - acc: 0.8014 - val_loss: 0.5447 - val_acc: 0.7630\n",
      "Epoch 19/25\n",
      "63/63 [==============================] - 20s 317ms/step - loss: 0.5336 - acc: 0.7535\n",
      "250/250 [==============================] - 139s 556ms/step - loss: 0.4110 - acc: 0.8054 - val_loss: 0.5336 - val_acc: 0.7535\n",
      "Epoch 20/25\n",
      "63/63 [==============================] - 20s 325ms/step - loss: 0.5568 - acc: 0.7430\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 0.4098 - acc: 0.8126 - val_loss: 0.5568 - val_acc: 0.7430\n",
      "Epoch 21/25\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.5350 - acc: 0.7635\n",
      "250/250 [==============================] - 117s 467ms/step - loss: 0.3910 - acc: 0.8225 - val_loss: 0.5350 - val_acc: 0.7635\n",
      "Epoch 22/25\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 0.5262 - acc: 0.7660\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.3840 - acc: 0.8290 - val_loss: 0.5262 - val_acc: 0.7660\n",
      "Epoch 23/25\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.5628 - acc: 0.7535\n",
      "250/250 [==============================] - 116s 466ms/step - loss: 0.3715 - acc: 0.8298 - val_loss: 0.5628 - val_acc: 0.7535\n",
      "Epoch 24/25\n",
      "63/63 [==============================] - 20s 312ms/step - loss: 0.5533 - acc: 0.7560\n",
      "250/250 [==============================] - 113s 454ms/step - loss: 0.3625 - acc: 0.8379 - val_loss: 0.5533 - val_acc: 0.7560\n",
      "Epoch 25/25\n",
      "63/63 [==============================] - 16s 253ms/step - loss: 0.5858 - acc: 0.7485\n",
      "250/250 [==============================] - 114s 455ms/step - loss: 0.3520 - acc: 0.8446 - val_loss: 0.5858 - val_acc: 0.7485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x232b0c487c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch = 8000,\n",
    "        epochs = 25,\n",
    "        validation_data = test_set,\n",
    "        validation_steps = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
