{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALISING CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST CONVOLUTION LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the image need to be provided in the 2d array format we need to provide the features and the matrix format.\n",
    "\n",
    "Need to Mention these in the convolution 2d matrix object.\n",
    "\n",
    "The basic picture features start from 32 and continues by multiplying with 2.\n",
    "\n",
    "Need to the mention the rows and columns of matrix format aswell.\n",
    "\n",
    "Borders, if not mentioned it will consider default format.\n",
    "\n",
    "Input shape (all images dont have same format)\n",
    "    * 3 be the shape of array (color image)\n",
    "    * 64 be the number of colors, can be increased twice depending on image\n",
    "    \n",
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) # 32 features of 3x3 matrix # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST LAYER POOLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to mention the matrix size of the pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND CONVOLUTION LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more convolution layers and pooling them helps in getting the more efficent results with higher proportion of success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(filters=32, kernel_size=3, activation='relu')) # For the second Convolution Layer there's no need of shape, as its already mentioned in first convolution layer #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND LAYER POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLATTENING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to mention the object for the flattening as the tensorflow keras will consider it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL CONNECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of units should be considered by trail & error.\n",
    "\n",
    "The Rectifier Activation function is used in to verify the data provided.\n",
    "\n",
    "The Sigmoid Activation function is used in order to classify the outputs. If there are two outputs then the activation should be of SIGMOID else SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of loss varies on the number of outcomes.\n",
    "    * If there are just two outcomes, we need to use \"binary_crossentropy\"\n",
    "    * If there are just more than two outcomes, we need to use \"categorical_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITTING CNN TO THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below template is taken from Keras Documentation Which provides all the data required for the output classification.\n",
    "\n",
    "The training and test data is splitted because of the easier processing of data.\n",
    "\n",
    "We need to edit the configurations of the data taken from keras as per our requirement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                        rescale = 1./255,\n",
    "                        shear_range = 0.2,\n",
    "                        zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "                        'G:/Education/Machine Learning/ML_CSV_FILES/CNN Dataset Cats & Dogs/training_set',\n",
    "                        target_size = (64, 64),\n",
    "                        batch_size = 32,\n",
    "                        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "                        'G:/Education/Machine Learning/ML_CSV_FILES/CNN Dataset Cats & Dogs/test_set',\n",
    "                        target_size = (64, 64),\n",
    "                        batch_size = 32,\n",
    "                        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "63/63 [==============================] - 58s 925ms/step - loss: 0.6018 - acc: 0.6810\n",
      "250/250 [==============================] - 380s 2s/step - loss: 0.6688 - acc: 0.5776 - val_loss: 0.6018 - val_acc: 0.6810\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 28s 438ms/step - loss: 0.5620 - acc: 0.7160\n",
      "250/250 [==============================] - 145s 578ms/step - loss: 0.5946 - acc: 0.6809 - val_loss: 0.5620 - val_acc: 0.7160\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 32s 503ms/step - loss: 0.5811 - acc: 0.6990\n",
      "250/250 [==============================] - 248s 992ms/step - loss: 0.5565 - acc: 0.7172 - val_loss: 0.5811 - val_acc: 0.6990\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 30s 477ms/step - loss: 0.5969 - acc: 0.6820\n",
      "250/250 [==============================] - 288s 1s/step - loss: 0.5213 - acc: 0.7458 - val_loss: 0.5969 - val_acc: 0.6820\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 24s 375ms/step - loss: 0.5065 - acc: 0.7590\n",
      "250/250 [==============================] - 215s 859ms/step - loss: 0.5010 - acc: 0.7552 - val_loss: 0.5065 - val_acc: 0.7590\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 27s 434ms/step - loss: 0.4987 - acc: 0.7585\n",
      "250/250 [==============================] - 237s 947ms/step - loss: 0.4746 - acc: 0.7751 - val_loss: 0.4987 - val_acc: 0.7585\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 28s 448ms/step - loss: 0.4882 - acc: 0.7785\n",
      "250/250 [==============================] - 249s 996ms/step - loss: 0.4504 - acc: 0.7890 - val_loss: 0.4882 - val_acc: 0.7785\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 31s 488ms/step - loss: 0.4728 - acc: 0.7715\n",
      "250/250 [==============================] - 249s 997ms/step - loss: 0.4333 - acc: 0.7954 - val_loss: 0.4728 - val_acc: 0.7715\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - 28s 451ms/step - loss: 0.4660 - acc: 0.8015\n",
      "250/250 [==============================] - 265s 1s/step - loss: 0.4189 - acc: 0.8064 - val_loss: 0.4660 - val_acc: 0.8015\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - 31s 485ms/step - loss: 0.4709 - acc: 0.7825\n",
      "250/250 [==============================] - 256s 1s/step - loss: 0.3964 - acc: 0.8200 - val_loss: 0.4709 - val_acc: 0.7825\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - 33s 526ms/step - loss: 0.5090 - acc: 0.7725\n",
      "250/250 [==============================] - 277s 1s/step - loss: 0.3730 - acc: 0.8354 - val_loss: 0.5090 - val_acc: 0.7725\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - 39s 624ms/step - loss: 0.4894 - acc: 0.7770\n",
      "250/250 [==============================] - 323s 1s/step - loss: 0.3641 - acc: 0.8369 - val_loss: 0.4894 - val_acc: 0.7770\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - 37s 581ms/step - loss: 0.4837 - acc: 0.7990\n",
      "250/250 [==============================] - 246s 984ms/step - loss: 0.3516 - acc: 0.8407 - val_loss: 0.4837 - val_acc: 0.7990\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - 19s 307ms/step - loss: 0.5002 - acc: 0.7855\n",
      "250/250 [==============================] - 213s 853ms/step - loss: 0.3289 - acc: 0.8515 - val_loss: 0.5002 - val_acc: 0.7855\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - 23s 368ms/step - loss: 0.5443 - acc: 0.7850\n",
      "250/250 [==============================] - 171s 684ms/step - loss: 0.3135 - acc: 0.8627 - val_loss: 0.5443 - val_acc: 0.7850\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - 24s 385ms/step - loss: 0.5673 - acc: 0.7770\n",
      "250/250 [==============================] - 213s 853ms/step - loss: 0.2987 - acc: 0.8715 - val_loss: 0.5673 - val_acc: 0.7770\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - 21s 336ms/step - loss: 0.5421 - acc: 0.7825\n",
      "250/250 [==============================] - 191s 765ms/step - loss: 0.2844 - acc: 0.8755 - val_loss: 0.5421 - val_acc: 0.7825\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 0.5592 - acc: 0.7835\n",
      "250/250 [==============================] - 179s 715ms/step - loss: 0.2641 - acc: 0.8880 - val_loss: 0.5592 - val_acc: 0.7835\n",
      "Epoch 19/25\n",
      "63/63 [==============================] - 22s 354ms/step - loss: 0.5711 - acc: 0.7890\n",
      "250/250 [==============================] - 154s 615ms/step - loss: 0.2480 - acc: 0.8956 - val_loss: 0.5711 - val_acc: 0.7890\n",
      "Epoch 20/25\n",
      "63/63 [==============================] - 22s 346ms/step - loss: 0.6484 - acc: 0.7790\n",
      "250/250 [==============================] - 170s 680ms/step - loss: 0.2322 - acc: 0.9061 - val_loss: 0.6484 - val_acc: 0.7790\n",
      "Epoch 21/25\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.5666 - acc: 0.7930\n",
      "250/250 [==============================] - 164s 657ms/step - loss: 0.2200 - acc: 0.9122 - val_loss: 0.5666 - val_acc: 0.7930\n",
      "Epoch 22/25\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.6242 - acc: 0.7820\n",
      "250/250 [==============================] - 143s 572ms/step - loss: 0.2128 - acc: 0.9158 - val_loss: 0.6242 - val_acc: 0.7820\n",
      "Epoch 23/25\n",
      "63/63 [==============================] - 16s 249ms/step - loss: 0.6421 - acc: 0.7860\n",
      "250/250 [==============================] - 151s 603ms/step - loss: 0.1894 - acc: 0.9247 - val_loss: 0.6421 - val_acc: 0.7860\n",
      "Epoch 24/25\n",
      "63/63 [==============================] - 16s 258ms/step - loss: 0.6805 - acc: 0.7630\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 0.1821 - acc: 0.9261 - val_loss: 0.6805 - val_acc: 0.7630\n",
      "Epoch 25/25\n",
      "63/63 [==============================] - 16s 252ms/step - loss: 0.6673 - acc: 0.7970\n",
      "250/250 [==============================] - 143s 573ms/step - loss: 0.1707 - acc: 0.9344 - val_loss: 0.6673 - val_acc: 0.7970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20ec6423e08>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch = 8000,\n",
    "        epochs = 25,\n",
    "        validation_data = test_set,\n",
    "        validation_steps = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
